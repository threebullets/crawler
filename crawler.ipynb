{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大数据项目\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 第一版程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load crawler_v1.py\n",
    "#!usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Aiming at:\n",
    "\n",
    "    Get html, extract meta data and title and save them in json.\n",
    "\n",
    "First version by yifyu@foxmail.com\n",
    "\n",
    "Input:\n",
    "    userid-url file in lines, split '\\t'\n",
    "    \n",
    "Output:\n",
    "    html files, with hopefully following infomation:\n",
    "        \n",
    "        title, \n",
    "        keywords, \n",
    "        description, \n",
    "        meta\n",
    "        \n",
    "    and saved in json files\n",
    "\n",
    "Need improvements:\n",
    "    1. Develop a Multi-Thread Approach\n",
    "    2. Managing Exceptions (E.g. Re-connection, Max waiting time) \n",
    "    3. Better anti-anti-crawler practice \n",
    "    4. Writing Logs\n",
    "    \n",
    "\"\"\"\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# usr-url file path\n",
    "FILE_PATH = 'E:\\\\BiheTech\\\\sample10000.csv'\n",
    "\n",
    "# denote output path\n",
    "OUTPUT_PATH = 'E:\\\\BiheTech\\\\result'\n",
    "\n",
    "# max sleep time(in sec)\n",
    "# uniform distributed (0,max sleep time)\n",
    "# set near to zero, get max speed\n",
    "# But under greater detection risk\n",
    "MAX_SLEEP_TIME = 0.1\n",
    "\n",
    "# Fake headers:\n",
    "# When Blocked by certain website,\n",
    "# change this and try again\n",
    "# for more kinds of headers, search online\n",
    "headers = {'User-Agent':'Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;360SE)',\n",
    "           'Referer':'https://www.baidu.com'}\n",
    "\n",
    "def getHtml(url):\n",
    "    # This function gets a page\n",
    "    # returns a html file in text\n",
    "    page = requests.get(url, headers = headers)\n",
    "    html = page.text\n",
    "    # html = page.content.decode('utf-8')\n",
    "    return html\n",
    "\n",
    "class MyUsrUrl(object):\n",
    "    # Establish a memory-friendly generator\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.fname, 'rt', encoding='utf-8'):\n",
    "            yield line.strip('\\n').split('\\t')\n",
    "            \n",
    "# read usr-url line by line\n",
    "# with minimal memory consumption            \n",
    "lines = MyUsrUrl(FILE_PATH)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    # get usr id\n",
    "    uid = line[0]\n",
    "    print('Current uid:')\n",
    "    print(uid)\n",
    "    \n",
    "    # get html file\n",
    "    url = line[1]\n",
    "    print('Current url:')\n",
    "    print(url)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        print('Getting html file...')\n",
    "        html = getHtml(url)\n",
    "    \n",
    "        # Extract meta and title data\n",
    "        print('Extracting meta and title data...')\n",
    "        print('Soup it...')\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        print('Getting meta list...')\n",
    "        meta = list(soup.find_all(name = 'meta')) \n",
    "        \n",
    "        print('Getting title...')\n",
    "        title = list(soup.find_all(name = 'title')) \n",
    "        \n",
    "        print('Getting description...')\n",
    "        try:\n",
    "            description = soup.find(attrs={\"name\":\"description\"})['content']  \n",
    "        except:\n",
    "            description = None\n",
    "            \n",
    "        print('Getting keywords...')\n",
    "        try:\n",
    "            keywords = soup.find(attrs={\"name\":\"keywords\"})['content']\n",
    "        except:\n",
    "            try:\n",
    "                keywords = soup.find(attrs={\"name\":\"keyword\"})['content']\n",
    "            except:\n",
    "                keywords = None\n",
    "                \n",
    "                \n",
    "        # Save data to file\n",
    "        result = {'uid':uid, \n",
    "                  'url':url,\n",
    "                  'title':str(title),\n",
    "                  'keywords':keywords,\n",
    "                  'description':description,\n",
    "                  'meta':str(meta),\n",
    "                  'html':html}\n",
    "        \n",
    "        print('Trying to save file...')\n",
    "        with open(file = OUTPUT_PATH + '\\\\' + uid + '-' \n",
    "                  + time.strftime(\"%Y%m%d%H%M%S\",time.localtime()) + '.json',\n",
    "                  mode = 'x', \n",
    "                  encoding = 'utf-8') as file_obj:\n",
    "            json.dump(result, file_obj)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "    \n",
    "    print('Time when completed:')\n",
    "    print(time.strftime(\"%Y-%m-%d-%H:%M:%S\",time.localtime()))\n",
    "    \n",
    "    # Random request to avoid detection\n",
    "    time.sleep(random.uniform(0,MAX_SLEEP_TIME))\n",
    "    \n",
    "    # count # of urls that have been processed\n",
    "    count += 1\n",
    "    print(str(count)+' urls have been processed.')\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "totaltime = time_end - time_start\n",
    "\n",
    "print('Mission completed.')\n",
    "print('time used: ' + str(totaltime) + ' sec')\n",
    "\n",
    "################### Load data by json ########################\n",
    "# dict = json.load(open(file))\n",
    "# dict['keywords']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这是第二版程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这是第三版程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这是第四版程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load crawler_v4.py\n",
    "#!usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Aiming at:\n",
    "\n",
    "    Get html, extract meta data and title and save them in json.\n",
    "\n",
    "First version by yifyu@foxmail.com\n",
    "\n",
    "Input:\n",
    "    userid-url file in lines, split '\\t'\n",
    "    \n",
    "Output:\n",
    "    html files, with hopefully following infomation:\n",
    "        \n",
    "        title, \n",
    "        keywords, \n",
    "        description, \n",
    "        meta\n",
    "        \n",
    "    and saved in json files\n",
    "\n",
    "Need improvements:\n",
    "    1. Managing Exceptions (E.g. Re-connection)\n",
    "    2. Better anti-anti-crawler practice\n",
    "    \n",
    "\"\"\"\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import threading\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# usr-url file path\n",
    "FILE_PATH = './sample100.csv'\n",
    "# file path for record of processed users and urls\n",
    "RECORD_PATH = 'record.pic'\n",
    "\n",
    "\n",
    "# denote output path\n",
    "OUTPUT_PATH = './result'\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.mkdir(OUTPUT_PATH)\n",
    "\n",
    "# max sleep time(in sec)\n",
    "# uniform distributed (0,max sleep time)\n",
    "# set near to zero, get max speed\n",
    "# But under greater detection risk\n",
    "MAX_SLEEP_TIME = 0.5\n",
    "# num of thread used\n",
    "THREAD_NUM = 5\n",
    "# time interval for self check. unit: s. Use lower value for debugging\n",
    "TIMER_INTER = 60\n",
    "# self check thread\n",
    "SELF_CHECK_TIMER = None\n",
    "\n",
    "# list for user agent candidates\n",
    "USR_AGENTS = [\n",
    "    \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3; rv:11.0) like Gecko\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "    \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11\",\n",
    "    \"Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\"\n",
    "]\n",
    "\n",
    "# disable logger from requests\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "# setting logger format\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\", datefmt='%a, %d %b %Y %H:%M:%S')\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.DEBUG)\n",
    "\n",
    "# logger for file log output\n",
    "fileHandler = logging.FileHandler('crawler.log')\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "fileHandler.setLevel(logging.INFO)\n",
    "rootLogger.addHandler(fileHandler)\n",
    "\n",
    "# logger for console log output\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "# uncomment the next line to set console logger level to INFO\n",
    "# consoleHandler.setLevel(logging.INFO)\n",
    "rootLogger.addHandler(consoleHandler)\n",
    "\n",
    "\n",
    "def getHtml(url):\n",
    "\n",
    "    # Fake headers:\n",
    "    # When Blocked by certain website,\n",
    "    # change this and try again\n",
    "    # for more kinds of headers, search online\n",
    "    headers = {'User-Agent': random.choice(USR_AGENTS),\n",
    "               'Referer': 'https://www.baidu.com'}\n",
    "\n",
    "    # This function gets a page\n",
    "    # returns a html file in text\n",
    "    page = requests.get(url, headers=headers, timeout=1)\n",
    "    html = page.text\n",
    "    # html = page.content.decode('utf-8')\n",
    "    return html\n",
    "\n",
    "\n",
    "def processUsrUrl(line):\n",
    "    # get usr id\n",
    "    uid = line[0]\n",
    "\n",
    "    # get html file\n",
    "    url = line[1]\n",
    "\n",
    "    try:\n",
    "        html = getHtml(url)\n",
    "    except requests.RequestException:\n",
    "        logging.warning(\"Request error for %s\" % line)\n",
    "        return\n",
    "\n",
    "    # Extract meta and title data\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    meta = list(soup.find_all(name='meta'))\n",
    "\n",
    "    title = list(soup.find_all(name='title'))\n",
    "\n",
    "    try:\n",
    "        description = soup.find(attrs={\"name\": \"description\"})['content']\n",
    "    except:\n",
    "        description = None\n",
    "\n",
    "    try:\n",
    "        keywords = soup.find(attrs={\"name\": \"keywords\"})['content']\n",
    "    except:\n",
    "        try:\n",
    "            keywords = soup.find(attrs={\"name\": \"keyword\"})['content']\n",
    "        except:\n",
    "            keywords = None\n",
    "\n",
    "    # Save data to file\n",
    "    result = {'uid': uid,\n",
    "              'url': url,\n",
    "              'title': str(title),\n",
    "              'keywords': keywords,\n",
    "              'description': description,\n",
    "              'meta': str(meta),\n",
    "              'html': html}\n",
    "\n",
    "    with open(file=os.path.join(OUTPUT_PATH, uid + '-'\n",
    "            + time.strftime(\"%Y%m%d%H%M%S\", time.localtime()) + '.json'),\n",
    "              mode='x',\n",
    "              encoding='utf-8') as file_obj:\n",
    "        json.dump(result, file_obj)\n",
    "\n",
    "\n",
    "    # Random request to avoid detection\n",
    "    time.sleep(random.uniform(0, MAX_SLEEP_TIME))\n",
    "\n",
    "\n",
    "class UsrUrlIter:\n",
    "    # Establish a memory-friendly, thread-safe generator\n",
    "    def __init__(self, fname, record_fname=RECORD_PATH):\n",
    "        self.file = open(fname, 'rt', encoding='utf-8')\n",
    "        try:\n",
    "            with open(RECORD_PATH, 'rb') as fin:\n",
    "                self.processed_record = pickle.load(fin)\n",
    "        except IOError:\n",
    "            self.processed_record = set()\n",
    "        self.lock = threading.Lock()\n",
    "        self.count = 0\n",
    "\n",
    "    def saveRecord(self):\n",
    "        with open(RECORD_PATH, 'wb') as fout:\n",
    "            pickle.dump(self.processed_record, fout, -1)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            with self.lock:\n",
    "                line = self.file.readline()\n",
    "                if line in self.processed_record:\n",
    "                    logging.debug('Duplicate found for %s' % line.strip('\\n'))\n",
    "                    continue\n",
    "                if not line:\n",
    "                    raise StopIteration\n",
    "                self.processed_record.add(line)\n",
    "                self.count += 1\n",
    "            yield line.strip('\\n').split('\\t')\n",
    "\n",
    "    def __del__(self):\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "def crawlerThread(lines):\n",
    "    for line in lines:\n",
    "        logging.debug('Processing line %s' % line)\n",
    "        try:\n",
    "            processUsrUrl(line)\n",
    "        except:\n",
    "            url = line[1]\n",
    "            with open(url, 'w') as fout:\n",
    "                fout.write(sys.exc_info()[0])\n",
    "            logging.warning('Unknow error for %s, error message saved' % line)\n",
    "\n",
    "\n",
    "\n",
    "def selfCheckThread(lines, prev_num):\n",
    "\t# thread for self check. Pringing information and save records of processed item.\n",
    "    if not threading.main_thread().is_alive():\n",
    "        exit()\n",
    "    global SELF_CHECK_TIMER\n",
    "    lines.saveRecord()\n",
    "    logging.info('%d processed totally, %d processed in the last %d seconds. Process record saved.'\n",
    "                 % (lines.count, lines.count - prev_num, TIMER_INTER))\n",
    "    SELF_CHECK_TIMER = threading.Timer(TIMER_INTER, selfCheckThread, args=(lines, lines.count))\n",
    "    SELF_CHECK_TIMER.start()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.info('Start crawling users and urls in %s' % FILE_PATH)\n",
    "    time_start = time.time()\n",
    "    # read usr-url line by line\n",
    "    # with minimal memory consumption\n",
    "    lines = UsrUrlIter(FILE_PATH)\n",
    "    logging.info('%d threads are used to crawl web pages' % THREAD_NUM)\n",
    "    threads = [threading.Thread(target=crawlerThread, args=(lines,)) for i in range(THREAD_NUM)]\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    logging.info('Starting self check thread...')\n",
    "    self_check_thread = threading.Thread(target=selfCheckThread, args=(lines, 0))\n",
    "    self_check_thread.start()\n",
    "    self_check_thread.join()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    logging.info('All crawling threads completed')\n",
    "    SELF_CHECK_TIMER.cancel()\n",
    "    lines.saveRecord()\n",
    "    totaltime = time.time() - time_start\n",
    "    logging.info('Mission completed, %d processed. time used: %s sec' % (lines.count, totaltime))\n",
    "\n",
    "\n",
    "'''\n",
    "############################## Load data by json in Windows ########################\n",
    "### in Windows, the text is likely to be coded by utf-8 but in str type already\n",
    "### If that's the case, convert it into bytes again and decode with following function:\n",
    "\n",
    "def mydecode(str):\n",
    "    return bytes((ord(i) for i in str)).decode('utf-8')\n",
    "\n",
    "def decodeJSON(file):\n",
    "    with open(file) as in_file:\n",
    "\t    d = json.load(in_file)\n",
    "    for key in d.keys():\n",
    "\t    d[key] = mydecode(d[key])\n",
    "\treturn d\n",
    "\t\n",
    "dic = decodeJSON(filename)\n",
    "'''\n",
    "\n",
    "'''\n",
    "################### Load data by json in Linux ########################\n",
    "### In linux, the encoding seems to work well\n",
    "\n",
    "# dict = json.load(open(file))\n",
    "\n",
    "# dict['keywords']\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 这是第五版程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load crawler_v5.py\n",
    "#!usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Aiming at:\n",
    "\n",
    "    Get html, extract meta data and title and save them in json.\n",
    "\n",
    "First version by yifyu@foxmail.com\n",
    "\n",
    "Input:\n",
    "    userid-url file in lines, split '\\t'\n",
    "    \n",
    "Output:\n",
    "    html files, with hopefully following infomation:\n",
    "        \n",
    "        title, \n",
    "        keywords, \n",
    "        description, \n",
    "        meta\n",
    "        \n",
    "    and saved in json files\n",
    "\n",
    "Need improvements:\n",
    "    1. Better anti-anti-crawler practice\n",
    "    \n",
    "\"\"\"\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import threading\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# usr-url file path\n",
    "FILE_PATH = './sample10000.csv'\n",
    "# file path for record of processed users and urls\n",
    "RECORD_PATH = 'record.pic'\n",
    "\n",
    "\n",
    "# denote output path\n",
    "OUTPUT_PATH = './result'\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.mkdir(OUTPUT_PATH)\n",
    "\n",
    "# max sleep time(in sec)\n",
    "# uniform distributed (0,max sleep time)\n",
    "# set near to zero, get max speed\n",
    "# But under greater detection risk\n",
    "MAX_SLEEP_TIME = 1\n",
    "# max time of waiting connection\n",
    "TIME_OUT = 60\n",
    "# max times of reconnecting\n",
    "MAX_RETRY_TIMES = 5\n",
    "# num of thread used\n",
    "THREAD_NUM = 5\n",
    "# time interval for self check. unit: s. Use lower value for debugging\n",
    "TIMER_INTER = 60\n",
    "# self check thread\n",
    "SELF_CHECK_TIMER = None\n",
    "\n",
    "# list for user agent candidates\n",
    "USR_AGENTS = [\n",
    "    \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3; rv:11.0) like Gecko\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "    \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11\",\n",
    "    \"Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\"\n",
    "]\n",
    "\n",
    "# disable logger from requests\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "# setting logger format\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\", \n",
    "                                 datefmt='%a, %d %b %Y %H:%M:%S')\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.DEBUG)\n",
    "\n",
    "# logger for file log output\n",
    "fileHandler = logging.FileHandler('crawler.log')\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "fileHandler.setLevel(logging.INFO)\n",
    "rootLogger.addHandler(fileHandler)\n",
    "\n",
    "# logger for console log output\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "# uncomment the next line to set console logger level to INFO\n",
    "# consoleHandler.setLevel(logging.INFO)\n",
    "rootLogger.addHandler(consoleHandler)\n",
    "\n",
    "\n",
    "def getHtml(url):\n",
    "\n",
    "    # Fake headers:\n",
    "    # When Blocked by certain website,\n",
    "    # change this and try again\n",
    "    # for more kinds of headers, search online\n",
    "    headers = {'User-Agent': random.choice(USR_AGENTS),\n",
    "               'Referer': 'https://www.baidu.com'}\n",
    "\n",
    "    # This function gets a page\n",
    "    # returns a html file in text\n",
    "    page = requests.get(url, headers=headers, timeout=TIME_OUT)\n",
    "    html = page.text\n",
    "    # html = page.content.decode('utf-8')\n",
    "    return html\n",
    "\n",
    "\n",
    "def processUsrUrl(line):\n",
    "    # get usr id\n",
    "    uid = line[0]\n",
    "\n",
    "    # get html file, try again if the connection fails\n",
    "    url = line[1]\n",
    "    html = None    \n",
    "    retryTimes = MAX_RETRY_TIMES\n",
    "    while retryTimes > 0:\n",
    "        try:\n",
    "            html = getHtml(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            logging.warning(\"Request error for %s, retrying...\" % line)\n",
    "            time.sleep(1)\n",
    "            retryTimes -= 1\n",
    "        else:\n",
    "            break\n",
    "    if html == None:\n",
    "        logging.warning(\"Request error for %s, retrying failed.\" % line)\n",
    "        return\n",
    "\n",
    "    # Extract meta and title data\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    meta = list(soup.find_all(name='meta'))\n",
    "    title = list(soup.find_all(name='title'))\n",
    "\n",
    "    try:\n",
    "        description = soup.find(attrs={\"name\": \"description\"})['content']\n",
    "    except:\n",
    "        description = None\n",
    "\n",
    "    try:\n",
    "        keywords = soup.find(attrs={\"name\": \"keywords\"})['content']\n",
    "    except:\n",
    "        try:\n",
    "            keywords = soup.find(attrs={\"name\": \"keyword\"})['content']\n",
    "        except:\n",
    "            keywords = None\n",
    "\n",
    "    # Save data to file\n",
    "    result = {'uid': uid,\n",
    "              'url': url,\n",
    "              'title': str(title),\n",
    "              'keywords': keywords,\n",
    "              'description': description,\n",
    "              'meta': str(meta),\n",
    "              'html': html}\n",
    "\n",
    "    with open(file=os.path.join(OUTPUT_PATH, uid + '-'\n",
    "            + time.strftime(\"%Y%m%d%H%M%S\", time.localtime()) + '.json'),\n",
    "              mode='x',\n",
    "              encoding='utf-8') as file_obj:\n",
    "        json.dump(result, file_obj)\n",
    "\n",
    "\n",
    "    # Random request to avoid detection\n",
    "    time.sleep(random.uniform(0, MAX_SLEEP_TIME))\n",
    "\n",
    "\n",
    "class UsrUrlIter:\n",
    "    # Establish a memory-friendly, thread-safe generator\n",
    "    def __init__(self, fname, record_fname=RECORD_PATH):\n",
    "        self.file = open(fname, 'rt', encoding='utf-8')\n",
    "        try:\n",
    "            with open(RECORD_PATH, 'rb') as fin:\n",
    "                self.processed_record = pickle.load(fin)\n",
    "        except IOError:\n",
    "            self.processed_record = set()\n",
    "        self.lock = threading.Lock()\n",
    "        self.count = 0\n",
    "\n",
    "    def saveRecord(self):\n",
    "        with open(RECORD_PATH, 'wb') as fout:\n",
    "            pickle.dump(self.processed_record, fout, -1)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            with self.lock:\n",
    "                line = self.file.readline()\n",
    "                if line in self.processed_record:\n",
    "                    logging.debug('Duplicate found for %s' % line.strip('\\n'))\n",
    "                    continue\n",
    "                if not line:\n",
    "                    raise StopIteration\n",
    "                self.processed_record.add(line)\n",
    "                self.count += 1\n",
    "            yield line.strip('\\n').split('\\t')\n",
    "\n",
    "    def __del__(self):\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "def crawlerThread(lines):\n",
    "    for line in lines:\n",
    "        logging.debug('Processing line %s' % line)\n",
    "        try:\n",
    "            processUsrUrl(line)\n",
    "        except:\n",
    "            url = line[1]\n",
    "            with open(url, 'w') as fout:\n",
    "                fout.write(sys.exc_info()[0])\n",
    "            logging.warning('Unknow error for %s, error message saved' % line)\n",
    "\n",
    "\n",
    "\n",
    "def selfCheckThread(lines, prev_num):\n",
    "\t# thread for self check. Pringing information and save records of processed item.\n",
    "    if not threading.main_thread().is_alive():\n",
    "        exit()\n",
    "    global SELF_CHECK_TIMER\n",
    "    lines.saveRecord()\n",
    "    logging.info('%d processed totally, %d processed in the last %d seconds. Process record saved.'\n",
    "                 % (lines.count, lines.count - prev_num, TIMER_INTER))\n",
    "    SELF_CHECK_TIMER = threading.Timer(TIMER_INTER, selfCheckThread, args=(lines, lines.count))\n",
    "    SELF_CHECK_TIMER.start()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.info('Start crawling users and urls in %s' % FILE_PATH)\n",
    "    time_start = time.time()\n",
    "    # read usr-url line by line\n",
    "    # with minimal memory consumption\n",
    "    lines = UsrUrlIter(FILE_PATH)\n",
    "    logging.info('%d threads are used to crawl web pages' % THREAD_NUM)\n",
    "    threads = [threading.Thread(target=crawlerThread, args=(lines,)) for i in range(THREAD_NUM)]\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    logging.info('Starting self check thread...')\n",
    "    self_check_thread = threading.Thread(target=selfCheckThread, args=(lines, 0))\n",
    "    self_check_thread.start()\n",
    "    self_check_thread.join()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    logging.info('All crawling threads completed')\n",
    "    SELF_CHECK_TIMER.cancel()\n",
    "    lines.saveRecord()\n",
    "    totaltime = time.time() - time_start\n",
    "    logging.info('Mission completed, %d processed. time used: %s sec' % (lines.count, totaltime))\n",
    "\n",
    "\n",
    "'''\n",
    "############################## Load data by json in Windows ########################\n",
    "### in Windows, the text is likely to be coded by utf-8 but in str type already\n",
    "### If that's the case, convert it into bytes again and decode with following function:\n",
    "\n",
    "def mydecode(str):\n",
    "    return bytes((ord(i) for i in str)).decode('utf-8')\n",
    "\n",
    "def decodeJSON(file):\n",
    "    with open(file) as in_file:\n",
    "\t    d = json.load(in_file)\n",
    "    for key in d.keys():\n",
    "\t    d[key] = mydecode(d[key])\n",
    "\treturn d\n",
    "\t\n",
    "dic = decodeJSON(filename)\n",
    "'''\n",
    "\n",
    "'''\n",
    "################### Load data by json in Linux ########################\n",
    "### In linux, the encoding seems to work well\n",
    "\n",
    "# dict = json.load(open(file))\n",
    "\n",
    "# dict['keywords']\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
